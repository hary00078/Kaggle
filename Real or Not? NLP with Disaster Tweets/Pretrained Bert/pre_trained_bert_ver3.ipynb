{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext.data as data\n",
    "import torchtext.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import Vectors \n",
    "from torchtext.data import TabularDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "sample = \"hello world\"\n",
    "sample_token = tokenizer.tokenize(sample)\n",
    "print(sample_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7592, 2088]\n"
     ]
    }
   ],
   "source": [
    "sample_index = tokenizer.convert_tokens_to_ids(sample_token)\n",
    "print(sample_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab = False,\n",
    "                  tokenize = tokenize_and_cut,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = TabularDataset.splits(path =\".\", train = \"./train_test_data/train_data_fin.csv\", test=\"./train_test_data/test_data_fin.csv\", \n",
    "                        format=\"csv\", fields=[(\"text\", TEXT), (\"label\", LABEL)], skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_data.split(split_ratio=0.8, random_state =random.seed(0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 6090\n",
      "Number of validation examples: 1523\n",
      "Number of testing examples: 3263\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "print(f\"Number of testing examples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': [10716, 2067, 2039], 'label': '0.0'}\n",
      "{'text': [1996, 2159, 1997, 1996, 3842, 23713, 12368, 5680, 2451, 2024, 2006, 7397, 17712, 3995, 26493, 8591, 26965, 19222, 12259, 5057, 17698, 4047, 4181, 11475, 12155, 20899], 'label': '0.0'}\n",
      "{'text': [3130, 15422, 6977, 18447, 2615, 2000, 3489, 5003, 4939, 4213, 2226, 29653, 2015, 3510, 5325, 4520, 11884, 10474, 11113, 24472], 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[0]))\n",
    "print(vars(train_data[1]))\n",
    "print(vars(test_data[17]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noel', 'back', 'up']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(vars(train_data[0])[\"text\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'0.0': 0, '1.0': 1})\n"
     ]
    }
   ],
   "source": [
    "LABEL.build_vocab(train_data)\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator  = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size = BATCH_SIZE, shuffle = True, sort=False,\n",
    "                                                            device = device)\n",
    "test_iterator = data.BucketIterator(test_data, batch_size=BATCH_SIZE, shuffle=False, sort=False,\n",
    "                                    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치의 개수 : 48\n",
      "검증 데이터의 미니 배치의 개수 : 12\n",
      "테스트 데이터의 미니 배치의 개수 : 26\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 데이터의 미니 배치의 개수 :\", len(train_iterator))\n",
    "print(\"검증 데이터의 미니 배치의 개수 :\", len(valid_iterator))\n",
    "print(\"테스트 데이터의 미니 배치의 개수 :\", len(test_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101, 16175, 26869,  4426,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "print(batch.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator  = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                            batch_size = BATCH_SIZE, shuffle = True, sort=False,\n",
    "                                                            device = device)\n",
    "test_iterator = data.BucketIterator(test_data, batch_size=BATCH_SIZE, shuffle=False, sort=False,\n",
    "                                    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLSTMSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()  \n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = True,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout,\n",
    "                          bias = True)\n",
    "        self.layernorm = nn.LayerNorm(hidden_dim*2)\n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "\n",
    "        hidden, _ = self.rnn(embedded)\n",
    "        hidden = hidden[:, -1, :]\n",
    "        hidden = self.layernorm(hidden)\n",
    "        output = self.out(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 3\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.654 | Train Acc: 62.21%\n",
      "\t Val. Loss: 0.549 |  Val. Acc: 75.68%\n",
      "Epoch: 02 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.580 | Train Acc: 71.98%\n",
      "\t Val. Loss: 0.490 |  Val. Acc: 78.85%\n",
      "Epoch: 03 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.474 | Train Acc: 78.88%\n",
      "\t Val. Loss: 0.422 |  Val. Acc: 80.76%\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.442 | Train Acc: 81.10%\n",
      "\t Val. Loss: 0.417 |  Val. Acc: 82.70%\n",
      "Epoch: 05 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.428 | Train Acc: 81.45%\n",
      "\t Val. Loss: 0.419 |  Val. Acc: 82.25%\n",
      "Epoch: 06 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.411 | Train Acc: 82.23%\n",
      "\t Val. Loss: 0.420 |  Val. Acc: 82.04%\n",
      "Epoch: 07 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.402 | Train Acc: 82.84%\n",
      "\t Val. Loss: 0.416 |  Val. Acc: 81.47%\n",
      "Epoch: 08 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.387 | Train Acc: 83.56%\n",
      "\t Val. Loss: 0.403 |  Val. Acc: 81.96%\n",
      "Epoch: 09 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.375 | Train Acc: 84.05%\n",
      "\t Val. Loss: 0.423 |  Val. Acc: 83.09%\n",
      "Epoch: 10 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.368 | Train Acc: 84.79%\n",
      "\t Val. Loss: 0.429 |  Val. Acc: 81.35%\n",
      "Epoch: 11 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.351 | Train Acc: 85.39%\n",
      "\t Val. Loss: 0.412 |  Val. Acc: 82.83%\n",
      "Epoch: 12 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.333 | Train Acc: 86.66%\n",
      "\t Val. Loss: 0.479 |  Val. Acc: 82.02%\n",
      "Epoch: 13 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.315 | Train Acc: 87.10%\n",
      "\t Val. Loss: 0.459 |  Val. Acc: 82.60%\n",
      "Epoch: 14 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.297 | Train Acc: 88.13%\n",
      "\t Val. Loss: 0.437 |  Val. Acc: 82.76%\n",
      "Epoch: 15 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.275 | Train Acc: 88.86%\n",
      "\t Val. Loss: 0.489 |  Val. Acc: 81.94%\n",
      "Epoch: 16 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.247 | Train Acc: 89.95%\n",
      "\t Val. Loss: 0.549 |  Val. Acc: 79.83%\n",
      "Epoch: 17 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.231 | Train Acc: 90.66%\n",
      "\t Val. Loss: 0.587 |  Val. Acc: 80.89%\n",
      "Epoch: 18 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.210 | Train Acc: 91.69%\n",
      "\t Val. Loss: 0.597 |  Val. Acc: 79.57%\n",
      "Epoch: 19 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.198 | Train Acc: 92.05%\n",
      "\t Val. Loss: 0.584 |  Val. Acc: 81.37%\n",
      "Epoch: 20 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.169 | Train Acc: 93.49%\n",
      "\t Val. Loss: 0.703 |  Val. Acc: 81.29%\n",
      "Epoch: 21 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.174 | Train Acc: 93.32%\n",
      "\t Val. Loss: 0.661 |  Val. Acc: 81.72%\n",
      "Epoch: 22 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.151 | Train Acc: 93.74%\n",
      "\t Val. Loss: 0.659 |  Val. Acc: 78.95%\n",
      "Epoch: 23 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.135 | Train Acc: 94.74%\n",
      "\t Val. Loss: 0.801 |  Val. Acc: 80.21%\n",
      "Epoch: 24 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.128 | Train Acc: 94.81%\n",
      "\t Val. Loss: 0.785 |  Val. Acc: 80.21%\n",
      "Epoch: 25 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.126 | Train Acc: 95.14%\n",
      "\t Val. Loss: 0.678 |  Val. Acc: 79.84%\n",
      "Epoch: 26 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.118 | Train Acc: 95.42%\n",
      "\t Val. Loss: 0.730 |  Val. Acc: 80.55%\n",
      "Epoch: 27 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.096 | Train Acc: 96.33%\n",
      "\t Val. Loss: 0.800 |  Val. Acc: 80.58%\n",
      "Epoch: 28 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.102 | Train Acc: 95.98%\n",
      "\t Val. Loss: 0.810 |  Val. Acc: 80.68%\n",
      "Epoch: 29 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.095 | Train Acc: 96.12%\n",
      "\t Val. Loss: 0.863 |  Val. Acc: 79.16%\n",
      "Epoch: 30 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.089 | Train Acc: 96.44%\n",
      "\t Val. Loss: 0.700 |  Val. Acc: 78.35%\n",
      "Epoch: 31 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.089 | Train Acc: 96.48%\n",
      "\t Val. Loss: 0.905 |  Val. Acc: 80.13%\n",
      "Epoch: 32 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.080 | Train Acc: 96.55%\n",
      "\t Val. Loss: 0.813 |  Val. Acc: 79.01%\n",
      "Epoch: 33 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.076 | Train Acc: 97.07%\n",
      "\t Val. Loss: 0.878 |  Val. Acc: 80.05%\n",
      "Epoch: 34 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.072 | Train Acc: 97.10%\n",
      "\t Val. Loss: 0.826 |  Val. Acc: 81.13%\n",
      "Epoch: 35 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.068 | Train Acc: 97.44%\n",
      "\t Val. Loss: 0.887 |  Val. Acc: 79.49%\n",
      "Epoch: 36 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.068 | Train Acc: 97.24%\n",
      "\t Val. Loss: 0.908 |  Val. Acc: 80.10%\n",
      "Epoch: 37 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.071 | Train Acc: 97.09%\n",
      "\t Val. Loss: 1.060 |  Val. Acc: 80.16%\n",
      "Epoch: 38 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.064 | Train Acc: 97.25%\n",
      "\t Val. Loss: 0.915 |  Val. Acc: 80.36%\n",
      "Epoch: 39 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.069 | Train Acc: 97.05%\n",
      "\t Val. Loss: 0.894 |  Val. Acc: 78.38%\n",
      "Epoch: 40 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.061 | Train Acc: 97.48%\n",
      "\t Val. Loss: 0.905 |  Val. Acc: 79.81%\n",
      "Epoch: 41 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.070 | Train Acc: 97.39%\n",
      "\t Val. Loss: 0.991 |  Val. Acc: 79.17%\n",
      "Epoch: 42 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.066 | Train Acc: 97.30%\n",
      "\t Val. Loss: 0.949 |  Val. Acc: 80.05%\n",
      "Epoch: 43 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.056 | Train Acc: 97.62%\n",
      "\t Val. Loss: 0.952 |  Val. Acc: 80.52%\n",
      "Epoch: 44 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.058 | Train Acc: 97.44%\n",
      "\t Val. Loss: 1.034 |  Val. Acc: 80.87%\n",
      "Epoch: 45 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.055 | Train Acc: 97.54%\n",
      "\t Val. Loss: 1.000 |  Val. Acc: 79.31%\n",
      "Epoch: 46 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.074 | Train Acc: 97.18%\n",
      "\t Val. Loss: 0.911 |  Val. Acc: 81.00%\n",
      "Epoch: 47 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.060 | Train Acc: 97.50%\n",
      "\t Val. Loss: 1.007 |  Val. Acc: 81.59%\n",
      "Epoch: 48 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.051 | Train Acc: 97.65%\n",
      "\t Val. Loss: 1.010 |  Val. Acc: 79.84%\n",
      "Epoch: 49 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.057 | Train Acc: 97.50%\n",
      "\t Val. Loss: 1.054 |  Val. Acc: 80.33%\n",
      "Epoch: 50 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.066 | Train Acc: 97.31%\n",
      "\t Val. Loss: 0.971 |  Val. Acc: 78.99%\n",
      "Epoch: 51 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.053 | Train Acc: 97.58%\n",
      "\t Val. Loss: 1.196 |  Val. Acc: 81.46%\n",
      "Epoch: 52 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.048 | Train Acc: 97.71%\n",
      "\t Val. Loss: 0.958 |  Val. Acc: 80.62%\n",
      "Epoch: 53 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.048 | Train Acc: 97.82%\n",
      "\t Val. Loss: 1.042 |  Val. Acc: 81.32%\n",
      "Epoch: 54 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.047 | Train Acc: 97.77%\n",
      "\t Val. Loss: 1.232 |  Val. Acc: 81.12%\n",
      "Epoch: 55 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.060 | Train Acc: 97.69%\n",
      "\t Val. Loss: 0.789 |  Val. Acc: 80.86%\n",
      "Epoch: 56 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.055 | Train Acc: 97.73%\n",
      "\t Val. Loss: 0.844 |  Val. Acc: 80.85%\n",
      "Epoch: 57 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.058 | Train Acc: 97.64%\n",
      "\t Val. Loss: 0.920 |  Val. Acc: 80.94%\n",
      "Epoch: 58 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.049 | Train Acc: 97.78%\n",
      "\t Val. Loss: 0.984 |  Val. Acc: 81.08%\n",
      "Epoch: 59 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.049 | Train Acc: 97.74%\n",
      "\t Val. Loss: 0.962 |  Val. Acc: 80.99%\n",
      "Epoch: 60 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.052 | Train Acc: 97.68%\n",
      "\t Val. Loss: 1.018 |  Val. Acc: 80.26%\n",
      "Epoch: 61 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.047 | Train Acc: 97.87%\n",
      "\t Val. Loss: 1.037 |  Val. Acc: 79.81%\n",
      "Epoch: 62 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.047 | Train Acc: 97.72%\n",
      "\t Val. Loss: 1.028 |  Val. Acc: 81.42%\n",
      "Epoch: 63 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.05%\n",
      "\t Val. Loss: 0.907 |  Val. Acc: 81.01%\n",
      "Epoch: 64 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.047 | Train Acc: 97.69%\n",
      "\t Val. Loss: 1.017 |  Val. Acc: 81.54%\n",
      "Epoch: 65 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.00%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 81.12%\n",
      "Epoch: 66 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.051 | Train Acc: 97.84%\n",
      "\t Val. Loss: 0.993 |  Val. Acc: 80.06%\n",
      "Epoch: 67 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.041 | Train Acc: 98.05%\n",
      "\t Val. Loss: 1.149 |  Val. Acc: 80.96%\n",
      "Epoch: 68 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.050 | Train Acc: 97.92%\n",
      "\t Val. Loss: 0.869 |  Val. Acc: 81.21%\n",
      "Epoch: 69 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.045 | Train Acc: 97.78%\n",
      "\t Val. Loss: 1.351 |  Val. Acc: 81.39%\n",
      "Epoch: 70 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.046 | Train Acc: 97.89%\n",
      "\t Val. Loss: 1.005 |  Val. Acc: 80.47%\n",
      "Epoch: 71 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.13%\n",
      "\t Val. Loss: 1.123 |  Val. Acc: 79.79%\n",
      "Epoch: 72 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.049 | Train Acc: 97.99%\n",
      "\t Val. Loss: 0.921 |  Val. Acc: 79.71%\n",
      "Epoch: 73 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.045 | Train Acc: 98.15%\n",
      "\t Val. Loss: 1.078 |  Val. Acc: 79.81%\n",
      "Epoch: 74 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.049 | Train Acc: 97.97%\n",
      "\t Val. Loss: 0.923 |  Val. Acc: 80.59%\n",
      "Epoch: 75 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.28%\n",
      "\t Val. Loss: 1.127 |  Val. Acc: 80.44%\n",
      "Epoch: 76 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.23%\n",
      "\t Val. Loss: 0.993 |  Val. Acc: 80.58%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.20%\n",
      "\t Val. Loss: 1.126 |  Val. Acc: 79.74%\n",
      "Epoch: 78 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.15%\n",
      "\t Val. Loss: 0.968 |  Val. Acc: 80.04%\n",
      "Epoch: 79 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.047 | Train Acc: 98.05%\n",
      "\t Val. Loss: 0.905 |  Val. Acc: 82.28%\n",
      "Epoch: 80 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.21%\n",
      "\t Val. Loss: 1.158 |  Val. Acc: 80.71%\n",
      "Epoch: 81 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.034 | Train Acc: 98.32%\n",
      "\t Val. Loss: 1.247 |  Val. Acc: 81.80%\n",
      "Epoch: 82 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.05%\n",
      "\t Val. Loss: 1.062 |  Val. Acc: 81.64%\n",
      "Epoch: 83 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.038 | Train Acc: 98.23%\n",
      "\t Val. Loss: 1.153 |  Val. Acc: 80.74%\n",
      "Epoch: 84 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.13%\n",
      "\t Val. Loss: 1.069 |  Val. Acc: 81.01%\n",
      "Epoch: 85 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.048 | Train Acc: 97.75%\n",
      "\t Val. Loss: 0.975 |  Val. Acc: 81.89%\n",
      "Epoch: 86 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.048 | Train Acc: 97.76%\n",
      "\t Val. Loss: 1.136 |  Val. Acc: 81.54%\n",
      "Epoch: 87 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.18%\n",
      "\t Val. Loss: 1.211 |  Val. Acc: 81.64%\n",
      "Epoch: 88 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.035 | Train Acc: 98.41%\n",
      "\t Val. Loss: 1.143 |  Val. Acc: 80.93%\n",
      "Epoch: 89 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.046 | Train Acc: 98.02%\n",
      "\t Val. Loss: 0.948 |  Val. Acc: 81.23%\n",
      "Epoch: 90 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.15%\n",
      "\t Val. Loss: 1.040 |  Val. Acc: 80.81%\n",
      "Epoch: 91 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.16%\n",
      "\t Val. Loss: 0.945 |  Val. Acc: 79.87%\n",
      "Epoch: 92 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.039 | Train Acc: 98.02%\n",
      "\t Val. Loss: 1.205 |  Val. Acc: 79.60%\n",
      "Epoch: 93 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.045 | Train Acc: 97.93%\n",
      "\t Val. Loss: 1.119 |  Val. Acc: 80.36%\n",
      "Epoch: 94 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.34%\n",
      "\t Val. Loss: 1.200 |  Val. Acc: 80.26%\n",
      "Epoch: 95 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.06%\n",
      "\t Val. Loss: 1.219 |  Val. Acc: 80.92%\n",
      "Epoch: 96 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.12%\n",
      "\t Val. Loss: 0.976 |  Val. Acc: 80.36%\n",
      "Epoch: 97 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.036 | Train Acc: 98.36%\n",
      "\t Val. Loss: 1.170 |  Val. Acc: 80.00%\n",
      "Epoch: 98 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.044 | Train Acc: 98.09%\n",
      "\t Val. Loss: 1.029 |  Val. Acc: 81.10%\n",
      "Epoch: 99 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.040 | Train Acc: 98.12%\n",
      "\t Val. Loss: 1.071 |  Val. Acc: 80.89%\n",
      "Epoch: 100 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.042 | Train Acc: 98.21%\n",
      "\t Val. Loss: 0.977 |  Val. Acc: 80.74%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        if not os.path.isdir(\"pre_trained_bert_ver3\"):\n",
    "            os.makedirs(\"pre_trained_bert_ver3\")\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './pre_trained_bert_ver3/textclassificator.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(model, iterator):\n",
    "    \n",
    "    model.eval()\n",
    "    result = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "            x = batch.text\n",
    "            predictions = model(x).squeeze(0)\n",
    "            predictions = torch.round(torch.sigmoid(predictions))\n",
    "            result += predictions\n",
    "    \n",
    "    return result\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./pre_trained_bert_ver3/textclassificator.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'0': 0})\n",
      "[tensor([1.], device='cuda:0'), tensor([1.], device='cuda:0'), tensor([1.], device='cuda:0'), tensor([0.], device='cuda:0'), tensor([1.], device='cuda:0'), tensor([1.], device='cuda:0'), tensor([0.], device='cuda:0'), tensor([0.], device='cuda:0'), tensor([0.], device='cuda:0'), tensor([0.], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "LABEL.build_vocab(test_data)\n",
    "print(LABEL.vocab.stoi)\n",
    "result = make_csv(model, test_iterator)\n",
    "print(result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       0\n",
      "4       1\n",
      "5       1\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n"
     ]
    }
   ],
   "source": [
    "final_result_csv = pd.DataFrame(result, columns=[\"target\"])\n",
    "final_result_csv[\"target\"] = final_result_csv[\"target\"].apply(lambda x : int(x.item()))\n",
    "print(final_result_csv[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   0\n",
       "1   2\n",
       "2   3\n",
       "3   9\n",
       "4  11"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = pd.read_csv(\"./train_test_data/test.csv\")\n",
    "test_csv = test_csv.drop([\"keyword\", \"location\", \"text\"], axis=\"columns\")\n",
    "test_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.concat([test_csv, final_result_csv], axis = 1)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"./result/pre_trained_bert_ver3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
