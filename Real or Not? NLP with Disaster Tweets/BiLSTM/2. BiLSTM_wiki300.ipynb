{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext.data as data\n",
    "import torchtext.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import Vectors \n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from gensim.models import KeyedVectors \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = Vectors(name=\"./enwiki_20180420_win10_300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, batch_first=True)\n",
    "LABEL = data.Field(sequential=False, batch_first = True, is_target = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_torchtext, test_data_torchtext = TabularDataset.splits(path =\".\", train = \"./train_test_data/train_data_fin.csv\", test=\"./train_test_data/test_data_fin.csv\", \n",
    "                        format=\"csv\", fields=[(\"text\", TEXT), (\"label\", LABEL)], skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data_torchtext, min_freq=5, vectors=vectors)\n",
    "LABEL.build_vocab(train_data_torchtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2802\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(TEXT.vocab.stoi))\n",
    "print(len(LABEL.vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu와 cuda 중 다음 기기로 학습함: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_torchtext, val_data_torchtext = train_data_torchtext.split(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, val_iterator  = data.BucketIterator.splits((train_data_torchtext, val_data_torchtext),\n",
    "                                                           batch_size = 64, shuffle = True, sort=False)\n",
    "test_iterator = data.BucketIterator(test_data_torchtext, batch_size=len(test_data_torchtext), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치의 개수 : 96\n",
      "검증 데이터의 미니 배치의 개수 : 24\n",
      "테스트 데이터의 미니 배치의 개수 : 1\n"
     ]
    }
   ],
   "source": [
    "print(\"훈련 데이터의 미니 배치의 개수 :\", len(train_iterator))\n",
    "print(\"검증 데이터의 미니 배치의 개수 :\", len(val_iterator))\n",
    "print(\"테스트 데이터의 미니 배치의 개수 :\", len(test_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p = 0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=n_layers, bias=True, batch_first=True, bidirectional=True)\n",
    "        self.out = nn.Linear(hidden_dim*2, n_classes, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        h_t = x[:, -1, :]\n",
    "        self.dropout(h_t)\n",
    "        logit = self.out(h_t)\n",
    "        return logit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(3, 300, vocab_size, 300, n_classes).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "sample = torch.LongTensor(5, 4).random_(0, 10).to(DEVICE)\n",
    "sample_out = model(sample)\n",
    "print(sample_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1)\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iterator):\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for batch in val_iterator:\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1)\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, y, reduction=\"sum\")\n",
    "        total_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "        \n",
    "    size = len(val_iterator.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2802, 300])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0744, -0.1637,  0.0042,  ...,  0.1437, -0.0184, -0.0637],\n",
       "        ...,\n",
       "        [-0.2492,  0.1043,  0.0766,  ..., -0.2167, -0.1619, -0.2181],\n",
       "        [-0.3468, -0.4618,  0.0253,  ...,  0.0488, -0.1717,  0.0123],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 0] val loss :  0.40 | val accuracy : 83.78\n",
      "[Epoch: 1] val loss :  0.39 | val accuracy : 85.36\n",
      "[Epoch: 2] val loss :  0.41 | val accuracy : 84.44\n",
      "[Epoch: 3] val loss :  0.45 | val accuracy : 83.59\n",
      "[Epoch: 4] val loss :  0.48 | val accuracy : 81.22\n",
      "[Epoch: 5] val loss :  0.52 | val accuracy : 82.40\n",
      "[Epoch: 6] val loss :  0.65 | val accuracy : 81.16\n",
      "[Epoch: 7] val loss :  0.58 | val accuracy : 81.48\n",
      "[Epoch: 8] val loss :  0.67 | val accuracy : 80.63\n",
      "[Epoch: 9] val loss :  0.63 | val accuracy : 80.70\n",
      "[Epoch: 10] val loss :  0.71 | val accuracy : 78.07\n",
      "[Epoch: 11] val loss :  0.66 | val accuracy : 79.05\n",
      "[Epoch: 12] val loss :  0.57 | val accuracy : 77.48\n",
      "[Epoch: 13] val loss :  0.82 | val accuracy : 77.81\n",
      "[Epoch: 14] val loss :  0.83 | val accuracy : 78.46\n",
      "[Epoch: 15] val loss :  0.93 | val accuracy : 79.05\n",
      "[Epoch: 16] val loss :  1.04 | val accuracy : 76.95\n",
      "[Epoch: 17] val loss :  0.92 | val accuracy : 78.07\n",
      "[Epoch: 18] val loss :  0.96 | val accuracy : 78.53\n",
      "[Epoch: 19] val loss :  1.08 | val accuracy : 77.22\n",
      "[Epoch: 20] val loss :  0.86 | val accuracy : 78.66\n",
      "[Epoch: 21] val loss :  1.11 | val accuracy : 77.61\n",
      "[Epoch: 22] val loss :  1.42 | val accuracy : 77.35\n",
      "[Epoch: 23] val loss :  1.47 | val accuracy : 77.81\n",
      "[Epoch: 24] val loss :  1.30 | val accuracy : 77.28\n",
      "[Epoch: 25] val loss :  1.22 | val accuracy : 77.48\n",
      "[Epoch: 26] val loss :  1.38 | val accuracy : 76.69\n",
      "[Epoch: 27] val loss :  1.26 | val accuracy : 77.15\n",
      "[Epoch: 28] val loss :  1.31 | val accuracy : 77.15\n",
      "[Epoch: 29] val loss :  1.09 | val accuracy : 77.68\n",
      "[Epoch: 30] val loss :  1.17 | val accuracy : 77.48\n",
      "[Epoch: 31] val loss :  1.30 | val accuracy : 77.54\n",
      "[Epoch: 32] val loss :  1.29 | val accuracy : 76.10\n",
      "[Epoch: 33] val loss :  1.25 | val accuracy : 77.02\n",
      "[Epoch: 34] val loss :  1.32 | val accuracy : 75.90\n",
      "[Epoch: 35] val loss :  1.47 | val accuracy : 77.41\n",
      "[Epoch: 36] val loss :  1.49 | val accuracy : 77.54\n",
      "[Epoch: 37] val loss :  1.35 | val accuracy : 77.08\n",
      "[Epoch: 38] val loss :  1.29 | val accuracy : 77.48\n",
      "[Epoch: 39] val loss :  1.61 | val accuracy : 76.69\n",
      "[Epoch: 40] val loss :  1.51 | val accuracy : 76.36\n",
      "[Epoch: 41] val loss :  1.41 | val accuracy : 76.30\n",
      "[Epoch: 42] val loss :  1.21 | val accuracy : 78.59\n",
      "[Epoch: 43] val loss :  1.51 | val accuracy : 76.95\n",
      "[Epoch: 44] val loss :  1.50 | val accuracy : 77.61\n",
      "[Epoch: 45] val loss :  1.38 | val accuracy : 77.87\n",
      "[Epoch: 46] val loss :  1.38 | val accuracy : 77.15\n",
      "[Epoch: 47] val loss :  1.26 | val accuracy : 77.94\n",
      "[Epoch: 48] val loss :  1.10 | val accuracy : 77.87\n",
      "[Epoch: 49] val loss :  1.47 | val accuracy : 77.81\n",
      "[Epoch: 50] val loss :  1.38 | val accuracy : 76.95\n",
      "[Epoch: 51] val loss :  1.53 | val accuracy : 77.02\n",
      "[Epoch: 52] val loss :  1.60 | val accuracy : 76.95\n",
      "[Epoch: 53] val loss :  1.69 | val accuracy : 76.89\n",
      "[Epoch: 54] val loss :  1.66 | val accuracy : 77.28\n",
      "[Epoch: 55] val loss :  1.79 | val accuracy : 77.02\n",
      "[Epoch: 56] val loss :  1.76 | val accuracy : 77.41\n",
      "[Epoch: 57] val loss :  1.78 | val accuracy : 77.81\n",
      "[Epoch: 58] val loss :  1.92 | val accuracy : 77.02\n",
      "[Epoch: 59] val loss :  1.19 | val accuracy : 76.56\n",
      "[Epoch: 60] val loss :  1.52 | val accuracy : 76.89\n",
      "[Epoch: 61] val loss :  1.27 | val accuracy : 76.89\n",
      "[Epoch: 62] val loss :  1.49 | val accuracy : 78.66\n",
      "[Epoch: 63] val loss :  1.43 | val accuracy : 77.61\n",
      "[Epoch: 64] val loss :  1.60 | val accuracy : 77.48\n",
      "[Epoch: 65] val loss :  1.68 | val accuracy : 77.41\n",
      "[Epoch: 66] val loss :  1.66 | val accuracy : 77.81\n",
      "[Epoch: 67] val loss :  1.95 | val accuracy : 75.44\n",
      "[Epoch: 68] val loss :  1.40 | val accuracy : 77.41\n",
      "[Epoch: 69] val loss :  1.69 | val accuracy : 76.63\n",
      "[Epoch: 70] val loss :  1.89 | val accuracy : 76.69\n",
      "[Epoch: 71] val loss :  1.93 | val accuracy : 77.15\n",
      "[Epoch: 72] val loss :  1.69 | val accuracy : 77.15\n",
      "[Epoch: 73] val loss :  1.48 | val accuracy : 77.61\n",
      "[Epoch: 74] val loss :  1.22 | val accuracy : 78.59\n",
      "[Epoch: 75] val loss :  1.61 | val accuracy : 77.68\n",
      "[Epoch: 76] val loss :  1.43 | val accuracy : 78.00\n",
      "[Epoch: 77] val loss :  1.59 | val accuracy : 77.35\n",
      "[Epoch: 78] val loss :  1.82 | val accuracy : 77.61\n",
      "[Epoch: 79] val loss :  1.34 | val accuracy : 78.40\n",
      "[Epoch: 80] val loss :  1.71 | val accuracy : 78.07\n",
      "[Epoch: 81] val loss :  1.67 | val accuracy : 77.74\n",
      "[Epoch: 82] val loss :  1.80 | val accuracy : 77.61\n",
      "[Epoch: 83] val loss :  1.85 | val accuracy : 77.28\n",
      "[Epoch: 84] val loss :  1.90 | val accuracy : 77.35\n",
      "[Epoch: 85] val loss :  2.03 | val accuracy : 78.27\n",
      "[Epoch: 86] val loss :  1.76 | val accuracy : 77.81\n",
      "[Epoch: 87] val loss :  1.99 | val accuracy : 78.14\n",
      "[Epoch: 88] val loss :  2.05 | val accuracy : 78.07\n",
      "[Epoch: 89] val loss :  2.02 | val accuracy : 78.07\n",
      "[Epoch: 90] val loss :  2.10 | val accuracy : 78.46\n",
      "[Epoch: 91] val loss :  2.07 | val accuracy : 78.27\n",
      "[Epoch: 92] val loss :  2.15 | val accuracy : 78.33\n",
      "[Epoch: 93] val loss :  2.09 | val accuracy : 78.33\n",
      "[Epoch: 94] val loss :  2.10 | val accuracy : 77.54\n",
      "[Epoch: 95] val loss :  2.20 | val accuracy : 78.20\n",
      "[Epoch: 96] val loss :  1.87 | val accuracy : 77.35\n",
      "[Epoch: 97] val loss :  2.22 | val accuracy : 78.20\n",
      "[Epoch: 98] val loss :  2.25 | val accuracy : 78.07\n",
      "[Epoch: 99] val loss :  2.15 | val accuracy : 77.22\n",
      "[Epoch: 100] val loss :  1.87 | val accuracy : 78.00\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs + 1):\n",
    "    train(model, optimizer, train_iterator)\n",
    "    val_loss, val_accuracy = evaluate(model, val_iterator)\n",
    "    \n",
    "    print(\"[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f\" % (epoch, val_loss, val_accuracy))\n",
    "    \n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"BiLSTM_wiki300\"):\n",
    "            os.makedirs(\"BiLSTM_wiki300\")\n",
    "        torch.save(model.state_dict(), \"./BiLSTM_wiki300/textclassificator.pt\")\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./BiLSTM_wiki300/textclassificator.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_result(model, test_iterator):\n",
    "    model.eval()\n",
    "    total_result = []\n",
    "    for batch in test_iterator:\n",
    "        x, y = batch.text.to(DEVICE), batch.label.to(DEVICE)\n",
    "        y.data.sub_(1)\n",
    "        logit = model(x)\n",
    "        result = logit.max(1)[1]\n",
    "        total_result += result\n",
    "        \n",
    "        \n",
    "    return total_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263\n",
      "[tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "fin_result = test_result(model, test_iterator)\n",
    "print(len(fin_result))\n",
    "print(fin_result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(1, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0'), tensor(0, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "print(fin_result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target\n",
      "0       1\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "5       1\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n"
     ]
    }
   ],
   "source": [
    "final_result_csv = pd.DataFrame(fin_result, columns=[\"target\"])\n",
    "final_result_csv[\"target\"] = final_result_csv[\"target\"].apply(lambda x : x.item())\n",
    "print(final_result_csv[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   0\n",
       "1   2\n",
       "2   3\n",
       "3   9\n",
       "4  11"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"./train_test_data/test.csv\")\n",
    "test = test.drop([\"keyword\", \"location\", \"text\"], axis=\"columns\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = pd.concat([test, final_result_csv], axis = 1)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"./result/result_BiLSTM_wiki300\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
